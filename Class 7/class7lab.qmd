---
title: "Class 7 Lab"
author: "Zainab Fatima (PID: A16880407)"
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering and dimensionality reduction methods. 

Let's start by making up some data (where we knoew there are clear groups) that we can use to test out differebt clustering methods. 


We can use the `rnorm()` function to help us here. 
```{r}
hist(rnorm(n=3000, mean = 3))
```

Make data `z` with two "clusters" 

```{r}
rnorm(30, mean= -3)
rnorm(30, mean = +3)
```
```{r}
 x <- c( rnorm(30, mean= -3), rnorm(30, mean = +3) )

z <- cbind(x = x, rev(x)) 
#cbind makes columns, stands for "column-bindng"
head(z)
plot(z)
```
> Q. How big is dataset z?

```{r}
nrow(z)
ncol(z)
```

## K-means clustering 

The main function in "base" R for K-means clustering is called `kmeans()`

```{r}
k <- kmeans(z, centers = 2) 
k
#centers = # of clusters, size = we made each cluster have 30 
#clustering vector = just a label that says which points are different
```

```{r}
attributes(k)
```
> Q. How many points lie in each cluster? 

```{r}
k$size
```

> Q. What component of our results tells us about the cluster membership (i.e. which point likes in which cluster)? 

```{r}
k$cluster
```
> Q. Center of each cluster?

```{r}
k$center
```

> Q. Put this result info together and make a little "base R" plot of our clustering result. Also add the cluster center points to this plot. 

```{r}
plot(z, col="blue") #makes a base figure of clustering results but we did not add center points yet
```

Alternating points can have different colors
```{r}
plot(z, col= c("blue","red")) 
```



You can color by number. 
```{r}
plot(z, col= c(1,2)) 
```

Plot colored by cluster membership. To make different clusters with different colors, we can use: 
```{r}
plot(z, col = k$cluster)
points(k$center, col = "blue", pch =15)
```

> Q. Run kmeans on our input `z` and define 4 clusters making the same result vizualization plot as above (plot of z colored by cluster membership)

```{r}
k4 <- kmeans(z, centers = 4)
```

```{r}
plot (z, col = k4$cluster)
points(k$center, col = "blue", pch =15) 
```

```{r}
#to measure how well the clustering was: 
k4$tot.withinss
```

## Hierarchical Clustering 

The main function in base R for this is called `hclust()` it will take as input a distance matriz (key point is that you can't just give your raw data as input - you have to first calculate a distance matrix from your data). 

```{r}
# distance matrix = distance from each point to every other point 
d <- dist(z)
hc <- hclust(d)
hc
```
```{r}
plot(hc)
#in the plot, one cluster is 1 - 30, other cluster is 30 - 60 
```

Once I inspect the "tree"/dendrogram I can "cut" the tree to yield my groupings or clusters. The function to this is called `cutree()`

```{r}
# h = height where tree is cut
grps <- cutree(hc, h= 10)
```

```{r}
plot(z, col=grps)
```

## Hands on with Principal Component Analysis (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similar and if so how?


## Data Import 
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```

> Q1. How many rows and columns are in your new dataset. Which R functions can you use?

```{r}
nrow(x)
ncol(x)
dim(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second approach—using read.csv(url, row.names=1)—is preferred because it's more concise. 

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
#changing beside = false
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
#if something is on a straight line, it's similar amount in both countries
```

Looking at these types of "pairwise plots" can be helpful but it does not scale well and is more time-consuming/error-prone. There must be a better way..

### PCA to the rescue!

> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main function for PCA in base R is called `prcomp`. This function wants the transpose of our input data - i.e. the important food categories in as columns and the countries as rows.

```{r}
#head(t(x)) shows transposed tables
pca <- prcomp(t(x))
summary (pca)
#PC1-4 are the new axis created 
# 96% of data captures by PC1 and PC2 so we can plot on PC1 and PC2
```

Let's see what is in our PCA result object `pca`
```{r}
attributes(pca)
```

The `pca$x` result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis" (aka "PCs", "eigenvectors", etc.)

```{r}
head(pca$x)
```


> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1], pca$x[,2], pch = 16,
     col=c("orange", "red", "blue", "darkgreen"),
     xlab = "PC1", ylab = "PC2")
# england = orange, wales = red, scotland = blue, north ireland = dark green
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], pch = 16,
     col=c("orange", "red", "blue", "darkgreen"),
     xlab = "PC1", ylab = "PC2")

text(pca$x[,1], pca$x[,2], labels = rownames(pca$x), 
     col=c("orange", "red", "blue", "darkgreen"), pos = 3)
```

Below we can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100)
v
```
```{r}
## or the second row here...
z <- summary(pca)
z$importance
```
Plotting variance with respect to the principal component number 

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


## Digging Deeper (variable loadings) 

We can look at the so-called PC "loadings" result object to see how the original foods contribute to our new PCs (ie how the original variables contribute to our new better variables). 

```{r}
pca$rotation[,1] #how much each variable contributes to PCA1
```
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2)
```

## Using ggplot for these figures 

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()
```

We can make this look nicer. 

```{r}
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country, label=Country) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_vline(xintercept = 0, col="gray") +
  geom_point(show.legend = FALSE) +
  geom_label(hjust=1, nudge_x = -10, show.legend = FALSE) +
  expand_limits(x = c(-300,500)) +
  xlab("PC1 (67.4%)") +
  ylab("PC2 (28%)") +
  theme_bw()
```

```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```

```{r}
ggplot(ld_lab) +
  aes(PC1, reorder(Food, PC1), bg=PC1) +
  geom_col() + 
  xlab("PC1 Loadings/Contributions") +
  ylab("Food Group") +
  scale_fill_gradient2(low="purple", mid="gray", high="darkgreen", guide=NULL) +
  theme_bw()
```

## Biplots 

```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```

